# https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/pendulum-ppo.yaml
# Can expect improvement to -140 reward in ~300-500k timesteps.

# https://docs.ray.io/en/master/rllib/rllib-training.html#common-parameters
# rllib train -f examples/yaml_files/pendulum-ppo.yaml
#  tensorboard --logdir G:\PycharmProject\aitimetable\Pendulum-v1\pendulum-ppo
pendulum-ppo:
    env: Pendulum-v1
    run: PPO
    stop:
        episode_reward_mean: -500
        timesteps_total: 400000
    config:
        # Works for both torch and tf.
        framework: tf2
        train_batch_size: 512
        vf_clip_param: 10.0
        num_workers: 0
        num_envs_per_worker: 20
        lambda: 0.1
        gamma:
            grid_search: [0.9, 0.95,0.99]
        lr: 0.0003
        sgd_minibatch_size: 64
        num_sgd_iter: 6
        observation_filter: MeanStdFilter
    local_dir: Pendulum-v1
    checkpoint_freq: 2
    keep_checkpoints_num: 3

# tensorboard --bind_all --port=12301 --logdir /home/ppirog/projects/rllib_examples/Pendulum-v1/pendulum-ppo
# kill tensorboard
# kill $(ps -e | grep 'tensorboard' | awk '{print $1}')
# # web browser address: http://212.109.128.252:12301/#scalars
# rllib train --run=PPO --env=BipedalWalkerHardcore-v2 --config='{"train_batch_size": 4000}'